# Video Captioning with BLIP-2: A Comprehensive Study

Welcome to the official repository accompanying our paper submission on enhancing video captioning performance using BLIP-2. This repository contains all the code, models, and documentation to replicate our study's findings, aiming to contribute to the advancement of video captioning techniques in the field of vision-language processing.

## Overview

Our research presents a detailed exploration of adapting BLIP-2, a versatile image-captioning model, for the task of video captioning. By focusing on model architecture, data utilization, and training methodologies, we demonstrate significant improvements in video captioning performance. This README provides an overview of the repository structure, setup instructions, and guidelines for replicating our experiments.

## Repository Structure

Our repository is built upon the LAVIS framework. We extend our sincere appreciation to the authors of LAVIS for making their codebase available, which serves as the foundation for our work. For more information on our project's configurations, visit LAVIS GitHub page: https://github.com/salesforce/LAVIS

## License

This project is released under the MIT License. See the [LICENSE](LICENSE) file for details.